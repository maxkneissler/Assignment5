---
title: "Assignment5"
author: "Max Knei√üler"
date: "28 1 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(rrot.dir = "C:/Users/user/Desktop/Uni Tuebingen",
      "/Masterstudium/1. Semester/Data Science Project Management/Assignment5")
```

## Introduction

The goal of this assignment to use a public Github repository to improve the
version control system and to manage the development history of this project
carefully. We will interact with the API of ticketmaster.com in a basic and an
advanced way to obtain data from event venues in Germany. We will make use of
this data by plotting a map of Germany, indicating the event locations in
Germany. In addition, this procedure is repeated for another European country.
I was working together with Elias Rudolph (Student ID: 5629871) and Nicolas 
Mollier (Student ID: 5640171)in this assignment to deepen the exchange about 
API and GitHub.

My public repository can be found by:
https://github.com/maxkneissler/Assignment5

To solve the assignment, the following packages are needed:

```{r library, warning = FALSE, message = FALSE}
library(jsonlite)
library(httr)
library(rlist)
library(tidyverse)
library(ggplot2)
```


## Exercise 1: Setting up a new GitHub repository

As I have not used GitHub before, I created an account and set up a repository
`Assignment5`, which can be accessed by the link presented in the introductory
part. In the following, when dealing with API requests, GitHub allows us to 
have a more efficient version control system. Through the local as well as the 
remote repository, we can access earlier versions easily if the results were not
desirable. Additionally, in case of computer problems, we know that the entire
history of the project is outsourced and can be easily accessed again by logging
into the Github account. Another benefit will not be used in this project, as 
I will not truly collaborate with a colleague. However, it is advisable to use
GitHub in collaborative projects to avoid miscommunications. 

I will proceed by staging smaller steps but committing just parts with a 
meaningful process such that the development history is not too crowded and 
the steps are comprehensive. Next, we will familiarize ourselves with the API 
structure of `ticketmaster.com`.  


## Exercise 2: Getting to know the API

Getting to know the structure of the API queries on the website of 
`ticketmaster.com`, we see that the URI-format requires a package, a version, an
api key and a resource. First, we make use of the API Explorer to obtain the 
access key, which is offered by the provider. The key is set up in a different 
R-file `api_tm` and treated secretly to maintain a high certain level of 
security and to get used to this file structure for personal keys on other APIs.
In addition, we have a look at the `robots.txt` website of ticketmaster.com, 
which clarifies what data should not be retrieved. However, we can conclude
that the data we are interested is not of big issue since the robots.txt-file
mostly deals with the disallowing of personal data and images.

```{r api_key, warning = FALSE, message = FALSE}
source("api_tm.R")
```


In this assignment, we will use the Discovery package as well as version v2, as 
recommended in the `Venue Search` part. In addition, we deal with a rate limit 
of 5,000 calls per day and a limit of 5 requests per second, which will be taken 
account for in the queries. In addition, the directory-based URL request seems
to be more appropriate.


## Exercise 3: Interacting with the API - the basics

Our first request deals with the content of event venues in Germany. Therefore,
we perform a GET-request to extract the desired data. In order to check the 
success of our query, we check the status before extracting the content.

```{r first_GET, warning = FALSE, message = FALSE}
# Apply the query
res_venuesGermany <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "DE",
                                   apikey = tm_api))

# Check the status
status_code(res_venuesGermany)

# Extract the content
contentTM <- fromJSON(content(res_venuesGermany, as = "text"))
```

We can see that `contentTM` includes the three subfolders `embedded`, `links`
and `page`. We can observe more than 4740 elements on 238 pages were imported. 
The folder `venues` on the other hand contains several subfolders like images,
city, country, name, type, id, etc. In order to follow the assignment, we are 
interested in extracting the seven characteristics `name`, `city`, `postalCode`,
`address`, `url`, `longitude` and `latitude`. In addition, some proper recoding
is conducted that the structure equals the output on the assignment sheet.

```{r char_extract, warning = FALSE, message = FALSE}
# Introducing the data frame
venue_data <- tibble(
  name = character(20),
  city = character(20),
  postalCode = character(20),
  address = character(20),
  url = character(20),
  longitude = character(20),
  latitude = character(20))

# Facilitate the extraction as all date lies in same folder
content <- contentTM[["_embedded"]][["venues"]]

# Fill the data frame with the respective data
venue_data$name <- content[["name"]]

venue_data$city <- content[["city"]][["name"]]
venue_data$city <- as.character(venue_data$city)

venue_data$postalCode <- content[["postalCode"]]

venue_data$address <- content[["address"]][["line1"]]
venue_data$address <- as.character(venue_data$address)

venue_data$url <- content[["url"]]

venue_data$latitude <- content[["location"]][["latitude"]] 
venue_data$latitude <- as.double(venue_data$latitude)

venue_data$longitude <- content[["location"]][["longitude"]]
venue_data$longitude <- as.double(venue_data$longitude)

# Glimpse at the final dataset
glimpse(venue_data)
```

As we see, it results, besides the first two entries, only missing values for
the longitude and the latitude of the venues. To check, what the issue is, we 
have a look at the particular `ID` of the venue to figure out if the issue
is due to our request or due to the data.

```{r extract_id, warning = FALSE, message = FALSE}
id_check <- as.data.frame(contentTM[["_embedded"]][["venues"]][["id"]])

res_id_check <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(id = id_check[1,1],
                                   apikey = tm_api))
contentID <- fromJSON(content(res_id_check, as = "text"))
```

By switching between the different `ID`, one can notice that the variable 
`location` of the response disappears for the the IDs after the first two
venue observations. Therefore, we can conclude that there is an issue with the
data and not our query. As we will now want to extract data from several pages,
we might end up with issues concerning the stacking of the information of the 
pages as the `location` variable vanishes.


## Exercise 4: Interacting with the API - advanced

As mentioned in the description of the GET-request, we could access about 4740 
elements from ticketmaster.com, however, we had a look on only the first page
so far. In order to access all possible observations, we make use of loop to
go through the different pages. First, we look at the exact amount of entries
as a flexible function since it is likely to vary over time. 

```{r count_entries, warning = FALSE, message = FALSE}
entries <- as.numeric(contentTM[["page"]][["totalElements"]])
pages <- floor(entries/100)  # number of complete pages
remainder <- entries - 100*pages
```

Then, we create another data frame with the entries, which we like to collect, 
and the corresponding size to accelerate the request. As an empty table is 
created, we start to loop through the different pages. We have to take care that
the default value of page is 0, so our loop has to start at 0 as well. We take
a size of 100 that we do not exceed the request limit per day. The remaining 
observations are added manually to prevent issues with filling the table. Like 
observed in exercise 3, it is not possible to extract the longitude and 
latitude characteristic for the German venues. Therefore, I exclude those
properties for the query. However, we will then not be able to draw a map with
the respective German venues.

```{r data_collection, warning = FALSE, message = FALSE}
# Creating the final file
venueDataComplete <- tibble(
  name = character(entries),
  city = character(entries),
  postalCode = character(entries),
  address = character(entries),
  url = character(entries),
# longitude = character(entries),
# latitude = character(entries)
)

# Introducing a temporary File
venueFilter <- tibble(
  name = character(100),
  city = character(100),
  postalCode = character(100),
  address = character(100),
  url = character(100),
# longitude = character(100),
# latitude = character(100)
  )

# Loop over all pages
for (page in 0:(pages-1)) {
  
res_venuesAll <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "DE",
                                    page = page,
                                    size = 100,
                                   apikey = tm_api))
status_code(res_venuesAll)
contentAll <- fromJSON(content(res_venuesAll, as = "text"))

# Extraction from same subfolder
content <- contentAll[["_embedded"]][["venues"]]
  
# Fill the data frame with the respective data
venueFilter$name <- content[["name"]]

venueFilter$city <- content[["city"]][["name"]]

venueFilter$postalCode <- content[["postalCode"]]

venueFilter$address <- content[["address"]][["line1"]]

venueFilter$url <- content[["url"]]

# Longitude and Latitude excluded to make the request work

# venueFilter$latitude <- content[["location"]][["latitude"]]
# venueFilter$longitude <- content[["location"]][["longitude"]]


# Filling into the final form (without longitude and latitude)
venueDataComplete[(100 * (page+1) - 99):(100*(page+1)),] <- venueFilter

# Prevent exceeding the rate limit
Sys.sleep(0.2)
}


# Introducing a temporary File
venueFilter <- tibble(
  name = character(remainder),
  city = character(remainder),
  postalCode = character(remainder),
  address = character(remainder),
  url = character(remainder),
# longitude = character(remainder),
# latitude = character(remainder)
  )


# Incomplete last page is added manually
page <- pages

res_venuesAll <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "DE",
                                    page = page,
                                    size = remainder,
                                   apikey = tm_api))
contentAll <- fromJSON(content(res_venuesAll, as = "text"))

# Extraction from same subfolder
content <- contentAll[["_embedded"]][["venues"]]
  
# Fill the data frame with the respective data
venueFilter$name <- content[["name"]]

venueFilter$city <- content[["city"]][["name"]]

venueFilter$postalCode <- content[["postalCode"]]

venueFilter$address <- content[["address"]][["line1"]]

venueFilter$url <- content[["url"]]

# Longitude and Latitude excluded to make the request work

# venueFilter$latitude <- content[["location"]][["latitude"]]
# venueFilter$longitude <- content[["location"]][["longitude"]]


# Filling into the final form (without longitude and latitude)
venueDataComplete[(100 * (pages+1) - 99):(entries),] <- venueFilter

head(venueDataComplete)
```


## Exercise 5: Visualizing the extracted data

We now should make use of the extracted data to visualize the venues of the 
different events in Germany. In the assignment, it is already given a map-code 
of Germany, which we use. However, as there are no longitude and latitude 
characteristics available, the German map remains empty.

```{r warning = FALSE, message = FALSE}
ggplot() +
  geom_polygon(
    aes(x = long, y = lat, group = group), 
                  data = map_data("world", region = "Germany"),
    fill = "grey90",color = "black") +
  theme_void() + coord_quickmap() +
  labs(title = "Event locations across Germany", 
                            caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
        plot.caption = element_text(face = "italic"))
#   geom_point(venueDataComplete, mapping = aes(longitude, latitude))
```


## Exercise 6: Event locations in other countries

In order to demonstrate that the request actually works, I will apply the 
query to Belgium and Italy to plot the venues in the respective map. Just a few
adjustments are necessary to obtain the desired result.

```{r Belgium, warning = FALSE, message = FALSE}
# Apply the query
res_venuesBELG <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "BE",
                                   apikey = tm_api))

# Extract the content
contentBE <- fromJSON(content(res_venuesBELG, as = "text"))



entries <- as.numeric(contentBE[["page"]][["totalElements"]])
pages <- floor(entries/100)  # number of complete pages
remainder <- entries - 100*(pages)




# Creating the final file
venueDataBE <- tibble(
  name = character(entries),
  city = character(entries),
  postalCode = character(entries),
  address = character(entries),
  url = character(entries),
  longitude = character(entries),
  latitude = character(entries)
)

# Introducing a temporary File
venueFilter <- tibble(
  name = character(100),
  city = character(100),
  postalCode = character(100),
  address = character(100),
  url = character(100),
  longitude = character(100),
  latitude = character(100)
  )

# Loop over all pages
for (page in 0:(pages-1)) {
  
res_venuesAll <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "BE",
                                    size = 100,
                                    page = page,
                                   apikey = tm_api))
status_code(res_venuesAll)
contentAll <- fromJSON(content(res_venuesAll, as = "text"))

# Extraction from same subfolder
content <- contentAll[["_embedded"]][["venues"]]
  

venueFilter$name <- content[["name"]]

venueFilter$city <- content[["city"]][["name"]]

venueFilter$postalCode <- content[["postalCode"]]

venueFilter$address <- content[["address"]][["line1"]]

venueFilter$url <- content[["url"]]

venueFilter$latitude <- content[["location"]][["latitude"]]
venueFilter$longitude <- content[["location"]][["longitude"]]

venueDataBE[(100 * (page+1) - 99):(100*(page+1)),] <- venueFilter

# Prevent exceeding the rate limit
Sys.sleep(0.2)
}

# Introducing a temporary File
venueFilter <- tibble(
  name = character(remainder),
  city = character(remainder),
  postalCode = character(remainder),
  address = character(remainder),
  url = character(remainder),
  longitude = character(remainder),
  latitude = character(remainder)
  )

# Incomplete last page is added manually
page <- pages

res_venuesAll <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "BE",
                                    page = page,
                                    size = remainder,
                                   apikey = tm_api))
contentAll <- fromJSON(content(res_venuesAll, as = "text"))

# Extraction from same subfolder
content <- contentAll[["_embedded"]][["venues"]]
  
# Fill the data frame with the respective data
venueFilter$name <- content[["name"]]

venueFilter$city <- content[["city"]][["name"]]

venueFilter$postalCode <- content[["postalCode"]]

venueFilter$address <- content[["address"]][["line1"]]

venueFilter$url <- content[["url"]]

venueFilter$latitude <- content[["location"]][["latitude"]]
venueFilter$longitude <- content[["location"]][["longitude"]]


# Filling into the final form (without longitude and latitude)
venueDataBE[(100 * (pages+1) - 99):(entries),] <- venueFilter
```

Now, we make use of the map to plot the corresponding venues into the map of 
Belgium.

```{r Belgium_plot, warning = FALSE, message = FALSE}
venueDataBE$longitude <- as.double(venueDataBE$longitude)
venueDataBE$latitude <- as.double(venueDataBE$latitude)

maxLatitude <- 51.456975083419564
minLatitude <- 49.820905125023124
minLongitude <- 2.580427185176589
maxLongitude <- 6.403329194705428

plotBE <- filter(venueDataBE, latitude < maxLatitude, latitude > minLatitude, 
                          longitude < maxLongitude, longitude > minLongitude)
ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group), 
                  data = map_data("world", region = "Belgium"),
                    fill = "grey90",color = "black") +
  geom_point(data = plotBE, aes(longitude, latitude), alpha = 0.2, col = "black") +
  theme_void() + 
  coord_quickmap() +
  labs(title = "Event locations across Belgium", 
                            caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
        plot.caption = element_text(face = "italic"))

```


We will apply the same procedure to Denmark.


```{r Denmark, warning = FALSE, message = FALSE}
# Apply the query
res_venuesDK <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "DK",
                                   apikey = tm_api))

# Extract the content
contentAT <- fromJSON(content(res_venuesDK, as = "text"))



entries <- as.numeric(contentAT[["page"]][["totalElements"]])
pages <- floor(entries/100)  # number of complete pages
remainder <- entries - 100*(pages)




# Creating the final file
venueDataDK <- tibble(
  name = character(entries),
  city = character(entries),
  postalCode = character(entries),
  address = character(entries),
  url = character(entries),
  longitude = character(entries),
  latitude = character(entries)
)

# Introducing a temporary File
venueFilter <- tibble(
  name = character(100),
  city = character(100),
  postalCode = character(100),
  address = character(100),
  url = character(100),
  longitude = character(100),
  latitude = character(100)
  )

# Loop over all pages
for (page in 0:(pages-1)) {
  
res_venuesAll <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "DK",
                                    size = 100,
                                    page = page,
                                   apikey = tm_api))
status_code(res_venuesAll)
contentAll <- fromJSON(content(res_venuesAll, as = "text"))

# Extraction from same subfolder
content <- contentAll[["_embedded"]][["venues"]]
  

venueFilter$name <- content[["name"]]

venueFilter$city <- content[["city"]][["name"]]

venueFilter$postalCode <- content[["postalCode"]]

venueFilter$address <- content[["address"]][["line1"]]

venueFilter$url <- content[["url"]]

venueFilter$latitude <- content[["location"]][["latitude"]]
venueFilter$longitude <- content[["location"]][["longitude"]]

venueDataDK[(100 * (page+1) - 99):(100*(page+1)),] <- venueFilter

# Prevent exceeding the rate limit
Sys.sleep(0.2)
}

# Introducing a temporary File
venueFilter <- tibble(
  name = character(remainder),
  city = character(remainder),
  postalCode = character(remainder),
  address = character(remainder),
  url = character(remainder),
  longitude = character(remainder),
  latitude = character(remainder)
  )

# Incomplete last page is added manually
page <- pages

res_venuesAll <- GET("https://app.ticketmaster.com/discovery/v2/venues",
                       query = list(countryCode = "DK",
                                    page = page,
                                    size = remainder,
                                   apikey = tm_api))
contentAll <- fromJSON(content(res_venuesAll, as = "text"))

# Extraction from same subfolder
content <- contentAll[["_embedded"]][["venues"]]
  
# Fill the data frame with the respective data
venueFilter$name <- content[["name"]]

venueFilter$city <- content[["city"]][["name"]]

venueFilter$postalCode <- content[["postalCode"]]

venueFilter$address <- content[["address"]][["line1"]]

venueFilter$url <- content[["url"]]

venueFilter$latitude <- content[["location"]][["latitude"]]
venueFilter$longitude <- content[["location"]][["longitude"]]


# Filling into the final form (without longitude and latitude)
venueDataDK[(100 * (pages+1) - 99):(entries),] <- venueFilter
```

As the data is extracted, we make once again use of the map to plot the borders
with the venues.

```{r Denmark_plot, warning = FALSE, message = FALSE}

venueDataDK$longitude <- as.double(venueDataDK$longitude)
venueDataDK$latitude <- as.double(venueDataDK$latitude)

maxLatitude <- 57.74530877878669
minLatitude <- 54.590310107590064
minLongitude <- 7.984077284485842
maxLongitude <- 13.120607292800624

plotDK <- filter(venueDataDK, latitude < maxLatitude, latitude > minLatitude, 
                          longitude < maxLongitude, longitude > minLongitude)


ggplot() +
  geom_polygon(
    aes(x = long, y = lat, group = group), 
                  data = map_data("world", region = "Denmark"),
    fill = "grey90",color = "black") +
    geom_point(data = plotDK, aes(longitude, latitude), alpha = 0.2) +
    theme_void() + 
    coord_quickmap() +
    labs(title = "Event locations across Denmark", 
                            caption = "Source: ticketmaster.com") +
    theme(title = element_text(size=8, face='bold'),
        plot.caption = element_text(face = "italic"))
```


